#!/usr/bin/env python3
# Copyright (c) Facebook, Inc. and its affiliates.
#
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.
"""
BLEU scoring of generated translations against reference translations.
"""

import argparse
import logging
import os
import re
import sys

logging.basicConfig(
    format="%(asctime)s | %(levelname)s | %(name)s | %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
    level=os.environ.get("LOGLEVEL", "INFO").upper(),
    stream=sys.stdout,
)
logger = logging.getLogger("fairseq_cli.score")

from fairseq.data import dictionary
from fairseq.scoring import bleu


def get_parser():
    parser = argparse.ArgumentParser(description="Command-line script for BLEU scoring.")
    # fmt: off
    parser.add_argument('-s', '--sys', default='-', help='system output')
    parser.add_argument('-r', '--ref', default=None, help='references')
    parser.add_argument('-o', '--order', default=4, metavar='N',
                        type=int, help='consider ngrams up to this order')
    parser.add_argument('--ignore-case', action='store_true',
                        help='case-insensitive scoring')
    parser.add_argument('--sacrebleu', action='store_true',
                        help='score with sacrebleu')
    parser.add_argument('--sentence-bleu', action='store_true',
                        help='report sentence-level BLEUs (i.e., with +1 smoothing)')
    # newly added arguments
    parser.add_argument('--score-files', type=str, nargs='+', default=None, help='scoring fairseq generated files')
    parser.add_argument("--metrics", type=str, nargs="+", default=None, help="score metrics")
    parser.add_argument("--trg-lang", type=str, default=None, help="target language used for deciding the best tokenize.")
    # fmt: on
    return parser


def cli_main():
    parser = get_parser()
    args = parser.parse_args()
    print(args)

    assert args.sys == "-" or os.path.exists(args.sys), "System output file {} does not exist".format(args.sys)

    if args.score_files is not None:
        from contextlib import contextmanager

        @contextmanager
        def _set_file_handler(filename):
            filehandler = logging.FileHandler(filename)
            logger.addHandler(filehandler)
            yield
            filehandler.close()
            logger.removeHandler(filehandler)

        if len(args.score_files) == 1 and args.score_files[0] == ".":
            from fairseq.file_io import PathManager

            args.score_files = sorted(filter(lambda x: x.endswith(".txt"), PathManager.ls(".")), key=os.path.getmtime)

        for filename in args.score_files:
            logger.info("Start scoring file: {}".format(filename))
            with _set_file_handler(filename):
                score_generated_file(args, filename)
        sys.exit()

    assert args.ref is not None and os.path.exists(args.ref), "Reference file {} does not exist".format(args.ref)

    dict = dictionary.Dictionary()

    def readlines(fd):
        for line in fd.readlines():
            if args.ignore_case:
                yield line.lower()
            else:
                yield line

    if args.sacrebleu:
        import sacrebleu

        def score(fdsys):
            with open(args.ref) as fdref:
                print(sacrebleu.corpus_bleu(fdsys, [fdref]).format())

    elif args.sentence_bleu:

        def score(fdsys):
            with open(args.ref) as fdref:
                scorer = bleu.Scorer(bleu.BleuConfig(pad=dict.pad(), eos=dict.eos(), unk=dict.unk()))
                for i, (sys_tok, ref_tok) in enumerate(zip(readlines(fdsys), readlines(fdref))):
                    scorer.reset(one_init=True)
                    sys_tok = dict.encode_line(sys_tok)
                    ref_tok = dict.encode_line(ref_tok)
                    scorer.add(ref_tok, sys_tok)
                    print(i, scorer.result_string(args.order))

    else:

        def score(fdsys):
            with open(args.ref) as fdref:
                scorer = bleu.Scorer(
                    bleu.BleuConfig(
                        pad=dict.pad(),
                        eos=dict.eos(),
                        unk=dict.unk(),
                    )
                )
                for sys_tok, ref_tok in zip(readlines(fdsys), readlines(fdref)):
                    sys_tok = dict.encode_line(sys_tok)
                    ref_tok = dict.encode_line(ref_tok)
                    scorer.add(ref_tok, sys_tok)
                print(scorer.result_string(args.order))

    if args.sys == "-":
        score(sys.stdin)
    else:
        with open(args.sys, "r") as f:
            score(f)


def score_generated_file(args, filename) -> None:
    """Specifically scoring files generated by fairseq_cli:generate"""
    import json

    _dict = dictionary.Dictionary()

    tgts, detok_tgts, hyps, detok_hyps = [], [], [], []

    def _split_line(line, maxsplit=1):
        return line.strip().split(maxsplit=maxsplit)[-1]

    with open(filename, "r", encoding="utf-8") as f:
        for line in f.readlines():
            if args.ignore_case:
                line = line.lower()

            if re.search(r"^T", line):
                tgts.append(_split_line(line, maxsplit=1))
            elif re.search(r"^DT", line):
                detok_tgts.append(_split_line(line, maxsplit=1))
            elif re.search(r"^H", line):
                hyps.append(_split_line(line, maxsplit=2))
            elif re.search(r"^DH", line):
                detok_hyps.append(_split_line(line, maxsplit=2))

    if "sacrebleu" in args.metrics:
        from sacrebleu import BLEU

        metric = BLEU(tokenize=None, trg_lang=args.trg_lang)

        results = metric.corpus_score(detok_hyps, [detok_tgts]).format()

        logger.info(f"sacre{results}")

    if "tokenized_bleu" in args.metrics:
        from sacrebleu import BLEU

        metric = BLEU(tokenize="none")

        results = metric.corpus_score(hyps, [tgts]).format()

        logger.info(f"tokenized-{results}")

    if "sentence_bleu" in args.metrics:
        from fairseq.scoring.bleu import BleuConfig, Scorer

        scorer = Scorer(BleuConfig(pad=_dict.pad(), eos=_dict.eos(), unk=_dict.unk()))
        # store scores for each sentence
        output_file = filename.replace(".txt", ".sentBLEU")

        scores = []
        with open(output_file, "w", encoding="utf-8") as f:
            for i, (sys_sent, ref_sent) in enumerate(zip(hyps, tgts)):
                scorer.reset(one_init=True)
                sys_tok = _dict.encode_line(sys_sent)
                ref_tok = _dict.encode_line(ref_sent)
                scorer.add(ref_tok, sys_tok)
                print(f"{i}\t{scorer.result_string(args.order)}", file=f)
                scores.append(scorer.score(args.order))

        averaged_score = sum(scores) / len(scores)
        logger.info(f"sentence-BLEU{args.order} (averaged) = {averaged_score:2.2f}")

    if "bertscore" in args.metrics:
        from fairseq.scoring.bertscore import BertScoreScorer, BertScoreScorerConfig

        scorer = BertScoreScorer(BertScoreScorerConfig(bert_score_lang=args.trg_lang))
        # store scores for each sentence
        output_file = filename.replace(".txt", ".bertscore")

        for ref, pred in zip(detok_tgts, detok_hyps):
            scorer.add_string(ref, pred)

        score, hash_ = scorer.score(return_hash=True)
        with open(output_file, "w", encoding="utf-8") as f:
            for s in score:
                f.write(str(s.item()) + "\n")

        import numpy as np

        logger.info(f"BERTScore: {np.mean(score.numpy()):.4f} | {hash_}")


if __name__ == "__main__":
    cli_main()
